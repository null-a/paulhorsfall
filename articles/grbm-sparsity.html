<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2021-04-26 Mon 16:09 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>GRBM Sparsity</title>
<meta name="generator" content="Org mode" />
<meta name="author" content="Paul Horsfall" />

<link rel="stylesheet" href="/css/sp.css" type="text/css" />
<script>var clicky_site_ids = clicky_site_ids || []; clicky_site_ids.push(101275299);</script>
<script async src="//static.getclicky.com/js"></script>
<script type="text/javascript">
/*
@licstart  The following is the entire license notice for the
JavaScript code in this tag.

Copyright (C) 2012-2019 Free Software Foundation, Inc.

The JavaScript code in this tag is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code in this tag.
*/
<!--/*--><![CDATA[/*><!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = "code-highlighted";
     elem.className   = "code-highlighted";
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]>*///-->
</script>
</head>
<body>
<div id="content">
<h1 class="title">GRBM Sparsity</h1>

<div id="outline-container-orgba0bac8" class="outline-2">
<h2 id="orgba0bac8">An Observation</h2>
<div class="outline-text-2" id="text-orgba0bac8">
<p>
The variance of the visible units of a Gaussian RBM with rectified
linear hidden units affects the sparsity<sup><a id="fnr.1" class="footref" href="#fn.1">1</a></sup> of the representation
learned when the model is trained with contrastive divergence.
</p>
</div>
</div>

<div id="outline-container-org5a0617d" class="outline-2">
<h2 id="org5a0617d">Demonstration</h2>
<div class="outline-text-2" id="text-org5a0617d">
<p>
To demonstrate this effect I trained a model with 400 hidden units on
natural image patches. (<a href="https://github.com/phorsfall/ml/tree/master/ex/grbm/sparsity.py">Source code</a>.) For each training run the
standard deviation of the visible units (&sigma;) was fixed. The visible
units were sampled from during training.<sup><a id="fnr.2" class="footref" href="#fn.2">2</a></sup>
</p>


<div class="figure">
<p><img src="images/grbm-sparsity-fig1.png" alt="grbm-sparsity-fig1.png" />
</p>
<p><span class="figure-number">Figure 1: </span>Figure 1</p>
</div>

<p>
Figure 1 (<a href="images/grbm-sparsity-fig1.png">full size</a>) shows how the sparsity in the hidden units
changes during training for several values of &sigma;. It's clear that for
this model increasing &sigma; increases the sparsity of the learned
representation.
</p>


<div class="figure">
<p><img src="images/grbm-sparsity-fig2.png" alt="grbm-sparsity-fig2.png" />
</p>
<p><span class="figure-number">Figure 2: </span>Figure 2</p>
</div>

<p>
In figure 2 (<a href="images/grbm-sparsity-fig2.png">full size</a>) each row shows the first ten features learned
for a particular setting of &sigma;. The rows are in order of ascending
variance or equivalently ascending sparsity. Features toward the top
of this figure tend to be short edge detectors or point filters.
Moving down the figure the point filters disappear and the edge
detectors become longer and broader.
</p>
</div>
</div>

<div id="outline-container-org99ef052" class="outline-2">
<h2 id="org99ef052">Possible Implications</h2>
<div class="outline-text-2" id="text-org99ef052">
<p>
When an RBM is used as an unsupervised feature extractor it is
sometimes desirable to learn a sparse representation. One way this can
be achieved is by adding an extra term to the optimization objective.
An alternative is to add noise to the visible units during training
and think of the visible variance as a meta-parameter for controlling
sparsity.
</p>

<p>
If instead the objective is to learn a good generative model it makes
sense to learn the visible variances. In such a setting, the
observation made here suggests that the learning rate and initial
values chosen for the variances could have an appreciable effect on
the learned model.
</p>

<p>
For example, if the variances are initialized to relatively high
values and a small learning rate is applied to them, then I would
expect the sampling noise present during the early part of training to
induce more sparsity in the hidden units than would be the case with
smaller initial values and a bigger learning rate.<sup><a id="fnr.3" class="footref" href="#fn.3">3</a></sup>
</p>
</div>
</div>
<div id="footnotes">
<h2 class="footnotes">Footnotes: </h2>
<div id="text-footnotes">

<div class="footdef"><sup><a id="fn.1" class="footnum" href="#fnr.1">1</a></sup> <div class="footpara"><p class="footpara">
For a model with rectified linear hidden units sparsity is the
fraction of hidden units that output a zero, averaged over training
cases.
</p></div></div>

<div class="footdef"><sup><a id="fn.2" class="footnum" href="#fnr.2">2</a></sup> <div class="footpara"><p class="footpara">
If the visible units are not sampled then the variance still
affects sparsity but in a less predictable way. Furthermore, the
features learned are more noisy, less local and harder to interpret
as meaningful features such as edge detectors.
</p></div></div>

<div class="footdef"><sup><a id="fn.3" class="footnum" href="#fnr.3">3</a></sup> <div class="footpara"><p class="footpara">
Perhaps it is this phenomenon that Alex Krizhevsky is
describing in <a href="http://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf">Learning Multiple Layers of Features from Tiny Images</a>:
"The trick to doing this correctly is to set the learning rate of
the standard deviations to a sufficiently low value. In our
experience, it should be about 100 to 1000 times smaller than the
learning rate of the weights. Failure to do so produces a lot of
point filters that never evolve into edge filters."
</p></div></div>


</div>
</div></div>
</body>
</html>
